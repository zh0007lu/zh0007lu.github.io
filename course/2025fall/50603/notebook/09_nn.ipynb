{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "95054e6c",
      "metadata": {
        "id": "95054e6c"
      },
      "source": [
        "# 9: Neural Networks\n",
        "\n",
        "This notebook uses **scikit‑learn**'s `MLPClassifier` and `MLPRegressor` for a clean, production-grade workflow:\n",
        "- Pipelines with `StandardScaler`\n",
        "- Training with early stopping\n",
        "- Learning curves and validation curves\n",
        "- Small `GridSearchCV` for hyperparameter tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab1d4861",
      "metadata": {
        "id": "ab1d4861"
      },
      "source": [
        "\n",
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52125585",
      "metadata": {
        "id": "52125585"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split, learning_curve, validation_curve, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score\n",
        "\n",
        "# plotting defaults\n",
        "plt.rcParams[\"figure.figsize\"] = (6,4)\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "rng = np.random.default_rng(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abccae9c",
      "metadata": {
        "id": "abccae9c"
      },
      "source": [
        "\n",
        "## Part A — Classification on Two Moons\n",
        "We'll train an `MLPClassifier` inside a `Pipeline` with standardization, visualize the decision boundary, and plot learning & validation curves.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate data\n",
        "X, y = make_moons(n_samples=1200, noise=0.25, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)"
      ],
      "metadata": {
        "id": "FA6SKPe1vmKt"
      },
      "id": "FA6SKPe1vmKt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training and test data\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=8, alpha=0.5, label=\"Train\", marker='^')\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=8, alpha=0.5, label=\"Test\", marker='o')\n",
        "plt.title(\"Training and Test Data\")\n",
        "plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QTmJ8ZZwv3rO"
      },
      "id": "QTmJ8ZZwv3rO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40c77fbb",
      "metadata": {
        "id": "40c77fbb"
      },
      "outputs": [],
      "source": [
        "# Define pipeline\n",
        "clf = Pipeline(steps=[\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"mlp\", MLPClassifier(hidden_layer_sizes=(64,64), activation=\"relu\",\n",
        "                          solver=\"adam\",\n",
        "                          alpha=1e-4,  # L2\n",
        "                          batch_size=128,\n",
        "                          learning_rate=\"adaptive\",\n",
        "                          max_iter=400,\n",
        "                          early_stopping=True,\n",
        "                          n_iter_no_change=20,\n",
        "                          random_state=42))\n",
        "])\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test accuracy: {acc:.3f}\")\n",
        "print()\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bb16d68",
      "metadata": {
        "id": "6bb16d68"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Decision boundary plot\n",
        "def plot_decision_regions(clf, X, y, steps=300, padding=0.6):\n",
        "    x_min, x_max = X[:,0].min()-padding, X[:,0].max()+padding\n",
        "    y_min, y_max = X[:,1].min()-padding, X[:,1].max()+padding\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, steps),\n",
        "                         np.linspace(y_min, y_max, steps))\n",
        "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "    zz = clf.predict(grid).reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, zz, alpha=0.3, levels=np.arange(-0.5, 2), antialiased=True)\n",
        "    plt.scatter(X[:,0], X[:,1], c=y, s=10, edgecolor=\"none\")\n",
        "    plt.title(\"MLPClassifier decision regions\")\n",
        "    plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
        "    plt.show()\n",
        "\n",
        "plot_decision_regions(clf, X_test, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss curve\n",
        "plt.plot(clf.named_steps['mlp'].loss_curve_)\n",
        "plt.title(\"MLPClassifier Loss Curve\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4sYk_fbWwZv8"
      },
      "id": "4sYk_fbWwZv8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dcdeaa0",
      "metadata": {
        "id": "5dcdeaa0"
      },
      "outputs": [],
      "source": [
        "# Small hyperparameter search\n",
        "param_grid = {\n",
        "    \"mlp__hidden_layer_sizes\": [(64,64), (128,), (128,64)],\n",
        "    \"mlp__alpha\": [1e-5, 1e-4, 1e-3],\n",
        "    \"mlp__activation\": [\"relu\", \"tanh\"]\n",
        "}\n",
        "\n",
        "search = GridSearchCV(clf, param_grid, cv=5, scoring=\"accuracy\", n_jobs=None)\n",
        "search.fit(X_train, y_train)\n",
        "print(\"Best params:\", search.best_params_)\n",
        "print(f\"Best CV accuracy: {search.best_score_:.3f}\")\n",
        "\n",
        "best_clf = search.best_estimator_\n",
        "test_acc = accuracy_score(y_test, best_clf.predict(X_test))\n",
        "print(f\"Test accuracy (tuned): {test_acc:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efc8cc18",
      "metadata": {
        "id": "efc8cc18"
      },
      "source": [
        "\n",
        "## Part B — Regression on a Sine Wave\n",
        "We'll fit a noisy sine curve with `MLPRegressor` and examine the fit.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create regression data\n",
        "n = 600\n",
        "Xs = rng.uniform(-3.0, 3.0, size=(n,1))\n",
        "ys = np.sin(1.5*Xs) + 0.2*rng.normal(size=(n,1))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(Xs, ys, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "JfzeTEDJxFsz"
      },
      "id": "JfzeTEDJxFsz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training and test data for regression\n",
        "plt.scatter(X_train[:, 0], y_train[:, 0], s=8, alpha=0.5, label=\"Train\")\n",
        "plt.scatter(X_test[:, 0], y_test[:, 0], s=8, alpha=0.5, label=\"Test\")\n",
        "plt.title(\"Training and Test Data (Regression)\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X-gn1uyXxIpD"
      },
      "id": "X-gn1uyXxIpD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e49beef",
      "metadata": {
        "id": "8e49beef"
      },
      "outputs": [],
      "source": [
        "reg = Pipeline(steps=[\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"mlp\", MLPRegressor(hidden_layer_sizes=(64,64),\n",
        "                         activation=\"tanh\", solver=\"adam\",\n",
        "                         alpha=1e-4, batch_size=128,\n",
        "                         learning_rate=\"adaptive\",\n",
        "                         max_iter=1000, early_stopping=True,\n",
        "                         n_iter_no_change=20,\n",
        "                         random_state=7))\n",
        "])\n",
        "\n",
        "reg.fit(X_train, y_train.ravel())\n",
        "\n",
        "y_pred = reg.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"Test MSE: {mse:.4f}, R^2: {r2:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e990922",
      "metadata": {
        "id": "8e990922"
      },
      "outputs": [],
      "source": [
        "# Plot the learned function\n",
        "xs = np.linspace(-3.2, 3.2, 400).reshape(-1,1)\n",
        "ys_true = np.sin(1.5*xs)\n",
        "ys_hat = reg.predict(xs)\n",
        "\n",
        "plt.scatter(X_train[:,0], y_train[:,0], s=8, alpha=0.5, label=\"train\")\n",
        "plt.scatter(X_test[:,0], y_test[:,0], s=8, alpha=0.5, label=\"test\")\n",
        "plt.plot(xs[:,0], ys_true[:,0], linewidth=2, label=\"true sin\")\n",
        "plt.plot(xs[:,0], ys_hat, linewidth=2, label=\"MLPRegressor\")\n",
        "plt.xlabel(\"x\"); plt.ylabel(\"y\")\n",
        "plt.title(\"Sine fit with MLPRegressor\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84a43b3b",
      "metadata": {
        "id": "84a43b3b"
      },
      "source": [
        "\n",
        "## Practical tips with scikit‑learn MLPs\n",
        "\n",
        "- **Always scale features** (e.g., `StandardScaler`), preferably via a `Pipeline`.\n",
        "- Use **`early_stopping=True`** with a reasonable `n_iter_no_change` to prevent overfitting and save time.\n",
        "- Try **`relu`** for classification; **`tanh`** can work well in regression with standardized inputs.\n",
        "- Tune **`alpha`** (L2) to control capacity; watch learning/validation curves.\n",
        "- For small tabular problems, start with modest widths (e.g., `(64,64)`), then scale up.\n",
        "- If the solver stalls, increase `max_iter` and consider `learning_rate=\"adaptive\"`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45c77d70",
      "metadata": {
        "id": "45c77d70"
      },
      "source": [
        "\n",
        "## Exercises\n",
        "\n",
        "1. Swap `activation` between `\"relu\"` and `\"tanh\"` and compare decision boundaries and test accuracy.\n",
        "2. Change `hidden_layer_sizes` to `(128,)` and `(128,64)`; examine learning curves.\n",
        "3. Add `max_fun` or increase `max_iter` to ensure convergence and compare performance.\n",
        "4. For regression, try `\"relu\"` activations and compare `R^2` against `\"tanh\"`.\n",
        "5. Extend the grid search with `mlp__learning_rate_init` and `mlp__beta_1`, `mlp__beta_2`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below trains a simple Multi-Layer Perceptron (MLP) model using PyTorch to classify data generated by the `make_moons` function from scikit-learn.\n",
        "\n",
        "1.  **Model Definition:**\n",
        "    *   A simple MLP with two hidden layers and ReLU activation is defined using `torch.nn.Module`.\n",
        "\n",
        "2.  **Training Setup:**\n",
        "    *   `nn.CrossEntropyLoss` is used as the loss function, suitable for multi-class classification.\n",
        "    *   `optim.Adam` is used as the optimizer to update the model's weights during training.\n",
        "\n",
        "3.  **Training Loop:**\n",
        "    *   The model is trained for a specified number of epochs.\n",
        "    *   For each epoch, the model iterates through the training data in batches, calculates the loss, and updates the weights using backpropagation.\n",
        "    *   The average loss for each epoch is printed.\n",
        "\n",
        "4.  **Decision Boundary Plot:**\n",
        "    *   After training, the code generates a plot showing the decision boundary learned by the MLP. This visually represents how the model classifies the data.\n",
        "    *   The test data points are also plotted to show how well the decision boundary separates the two classes."
      ],
      "metadata": {
        "id": "wqp6vvs1QyaY"
      },
      "id": "wqp6vvs1QyaY"
    },
    {
      "cell_type": "code",
      "source": [
        "# Reuse the data generation above\n",
        "X, y = make_moons(n_samples=1200, noise=0.25, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)"
      ],
      "metadata": {
        "id": "nXRIieivRITb"
      },
      "id": "nXRIieivRITb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Convert to tensors\n",
        "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test_t  = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_t  = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "train_ds = TensorDataset(X_train_t, y_train_t)\n",
        "test_ds  = TensorDataset(X_test_t, y_test_t)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
        "test_loader  = DataLoader(test_ds, batch_size=256)"
      ],
      "metadata": {
        "id": "L6Z0bbKcRVoM"
      },
      "id": "L6Z0bbKcRVoM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Model ---\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim=2, hidden_sizes=(64,64), out_dim=2):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden_sizes[0]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_sizes[1], out_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = MLP()"
      ],
      "metadata": {
        "id": "4kK8gVJVRqE7"
      },
      "id": "4kK8gVJVRqE7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training setup ---\n",
        "criterion = nn.CrossEntropyLoss()  # same loss as MLPClassifier\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)"
      ],
      "metadata": {
        "id": "jt_KbbCtRsia"
      },
      "id": "jt_KbbCtRsia",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training loop ---\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    model.train()  # Set model to train mode\n",
        "    total_loss = 0.0\n",
        "    for xb, yb in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * xb.size(0)\n",
        "    avg_loss = total_loss / len(train_loader.dataset)\n",
        "    if (epoch+1) % 10 == 0 or epoch == 0:\n",
        "        print(f\"Epoch {epoch+1:02d}: loss={avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "J5JnUDV9Rvbl"
      },
      "id": "J5JnUDV9Rvbl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Evaluation ---\n",
        "model.eval()  # Set model to evaluation mode\n",
        "with torch.no_grad():  # Disable gradient calculations\n",
        "    logits = model(X_test_t)\n",
        "    _, predicted = torch.max(logits, 1)\n",
        "    accuracy = (predicted == y_test_t).sum().item() / len(y_test_t)\n",
        "\n",
        "print(f\"Test accuracy: {accuracy:.3f}\")"
      ],
      "metadata": {
        "id": "kaPsqFJ_R0gD"
      },
      "id": "kaPsqFJ_R0gD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Decision boundary plot ---\n",
        "import numpy as np\n",
        "xx, yy = np.meshgrid(\n",
        "    np.linspace(X[:,0].min()-0.6, X[:,0].max()+0.6, 300),\n",
        "    np.linspace(X[:,1].min()-0.6, X[:,1].max()+0.6, 300)\n",
        ")\n",
        "grid = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n",
        "with torch.no_grad():\n",
        "    Z = model(grid).argmax(dim=1).numpy()\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.contourf(xx, yy, Z, alpha=0.3, levels=np.arange(-0.5,2), antialiased=True)\n",
        "plt.scatter(X_test[:,0], X_test[:,1], c=y_test, s=10, edgecolor=\"none\")\n",
        "plt.title(\"PyTorch MLP decision boundary\")\n",
        "plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L3tTCDVJSE7F"
      },
      "id": "L3tTCDVJSE7F",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}