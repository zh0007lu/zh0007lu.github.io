{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Use if you run the notebook on Google colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "metadata": {
        "id": "SD2CxjDgLezS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mglearn"
      ],
      "metadata": {
        "id": "Qr-xPrgTLftK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VK482nO8LRhc"
      },
      "source": [
        "# 2: $k$-Nearest Neighbours\n",
        "\n",
        "> If two things are similar, the thought of one will tend to trigger the thought of the other <br>\n",
        "-- Aristotle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tb95SlZdLRhg"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OENquhvLRhg"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import IPython\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import HTML\n",
        "\n",
        "sys.path.append(\"/content/drive/MyDrive/50603/code\")\n",
        "os.chdir('/content/drive/MyDrive/50603')\n",
        "\n",
        "import ipywidgets as widgets\n",
        "import mglearn\n",
        "from IPython.display import display\n",
        "from ipywidgets import interact, interactive\n",
        "from plotting_functions import *\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.model_selection import cross_validate, train_test_split\n",
        "from utils import *\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3ns5brkLRhh"
      },
      "source": [
        "<br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yy7ShQ-MLRhi"
      },
      "source": [
        "### Quick recap\n",
        "\n",
        "- Why do we split the data?\n",
        "- What are the advantages of cross-validation?\n",
        "- What is overfitting?\n",
        "- What's the fundamental trade-off in supervised machine learning?\n",
        "- Types of data for our purposes\n",
        "  - Categorical:\n",
        "    - Nominal (sometimes just called *categorical*!), Ordinal\n",
        "  - Numerical:\n",
        "    - Discrete, Continuous"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcWWxJFDLRhi"
      },
      "source": [
        "## Learning outcomes\n",
        "\n",
        "From this lecture, you will be able to\n",
        "\n",
        "- explain the notion of similarity-based algorithms;\n",
        "- broadly describe how $k$-NNs use distances;\n",
        "- discuss the effect of using a small/large value of the hyperparameter $k$ when using the $k$-NN algorithm;\n",
        "- describe the problem of curse of dimensionality;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWwrXdP7LRhi"
      },
      "source": [
        "## Motivation and distances [[video](https://youtu.be/hCa3EXEUmQk)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koOE7jndLRhj"
      },
      "source": [
        "### Instance-based models\n",
        "\n",
        "- Suppose you are given the following training examples with corresponding labels and are asked to label a given test example.\n",
        "\n",
        "<!-- <img src='./img/knn-motivation.png' width=\"1000\"> -->\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "p = 'img/knn-motivation.png'\n",
        "display(Image(filename=p, width=1000))"
      ],
      "metadata": {
        "id": "alYuoizgMm5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[source](https://vipl.ict.ac.cn/en/database.php)\n",
        "\n",
        "- An intuitive way to classify the test example is by finding the most \"similar\" example(s) from the training set and using that label for the test example."
      ],
      "metadata": {
        "id": "kSZ5oOHiMhiH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3ZORuUeLRhj"
      },
      "source": [
        "### General idea of $k$-nearest neighbours algorithm\n",
        "\n",
        "- Consider the following toy dataset with two classes.\n",
        "    - blue circles $\\rightarrow$ class 0\n",
        "    - red triangles $\\rightarrow$ class 1\n",
        "    - green stars $\\rightarrow$ test examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbLqTHyOLRhj"
      },
      "outputs": [],
      "source": [
        "X, y = mglearn.datasets.make_forge()\n",
        "X_test = np.array([[8.2, 3.7], [9.9, 3.2], [11.2, 0.5]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hRUvi-MLRhk"
      },
      "outputs": [],
      "source": [
        "# plot_train_test_points is defined in code/plotting_functions.py\n",
        "\n",
        "plot_train_test_points(X, y, X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfvDWVwuLRhl"
      },
      "source": [
        "- Given a new data point, predict the class of the data point by finding the \"closest\" data point in the training set, i.e., by finding its \"nearest neighbour\" or majority vote of nearest neighbours."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtXUg5eTLRhl"
      },
      "outputs": [],
      "source": [
        "# plot_knn_clf is defined in code/plotting_functions.py\n",
        "\n",
        "def f(n_neighbors):\n",
        "    return plot_knn_clf(X, y, X_test, n_neighbors=n_neighbors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IPFGI64LRhl"
      },
      "outputs": [],
      "source": [
        "interactive(\n",
        "    f,\n",
        "    n_neighbors=widgets.IntSlider(min=1, max=7, step=2, value=1),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "02fy6UVsLRhl"
      },
      "source": [
        "### **Geometric view** of tabular data and dimensions\n",
        "\n",
        "- To understand instance-based algorithms it's useful to think of <u>data as points</u> in a high dimensional space.\n",
        "- Our `X` represents the the problem in terms of relevant **features** ($d$) with one dimension for each **feature** (column).\n",
        "- Examples are **points in a $d$-dimensional space**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_j5RdMULRhl"
      },
      "source": [
        "How many dimensions (features) are there in the cities data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtH8Ix2yLRhm"
      },
      "outputs": [],
      "source": [
        "cities_df = pd.read_csv(\"data/canada_usa_cities.csv\")\n",
        "X_cities = cities_df[[\"longitude\", \"latitude\"]]\n",
        "y_cities = cities_df[\"country\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46IvTXY_LRhm"
      },
      "outputs": [],
      "source": [
        "mglearn.discrete_scatter(X_cities.iloc[:, 0], X_cities.iloc[:, 1], y_cities)\n",
        "plt.xlabel(\"longitude\")\n",
        "plt.ylabel(\"latitude\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "jRqnIBWcLRhm"
      },
      "source": [
        "### Dimensions in ML problems\n",
        "\n",
        "In ML, usually we deal with high dimensional problems where examples are hard to visualize.  \n",
        "\n",
        "- $d \\approx 20$ is considered low dimensional\n",
        "- $d \\approx 1000$ is considered medium dimensional\n",
        "- $d \\approx 100,000$ is considered high dimensional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuAxYtcALRhm"
      },
      "source": [
        "### Feature vectors\n",
        "\n",
        "**Feature vector** is composed of feature values associated with an example.\n",
        "\n",
        "They correspond to <u>rows</u> of our dataframes.\n",
        "\n",
        "Some example feature vectors are shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "at89ijNaLRhm"
      },
      "outputs": [],
      "source": [
        "print(\"\\nAn example feature vector from the cities dataset: %s\"\n",
        "      % (X_cities.iloc[0].to_numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "zGTXdzqDLRhm"
      },
      "source": [
        "### **Similarity** between examples\n",
        "\n",
        "Let's take 2 points (two feature vectors) from the cities dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bA7E_vj5LRhm"
      },
      "outputs": [],
      "source": [
        "two_cities = X_cities.sample(2, random_state=120)\n",
        "two_cities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpyK32jQLRhn"
      },
      "outputs": [],
      "source": [
        "# also recall that y was the country for each city\n",
        "y_cities.unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2R39pQGLRhn"
      },
      "source": [
        "The two sampled points are shown as big black circles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_wFGOC6LRhn"
      },
      "outputs": [],
      "source": [
        "mglearn.discrete_scatter(\n",
        "    X_cities.iloc[:, 0], X_cities.iloc[:, 1], y_cities, s=8, alpha=0.3\n",
        ")\n",
        "mglearn.discrete_scatter(\n",
        "    two_cities.iloc[:, 0], two_cities.iloc[:, 1], markers=\"o\", c=\"k\", s=18\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "5hQFxJ9CLRhn"
      },
      "source": [
        "### Distance between feature vectors\n",
        "\n",
        "- For the `two_cities` at the two big circles, what is the _distance_ between them?\n",
        "- A common way to calculate the distance between vectors is calculating the **Euclidean distance**.\n",
        "- The euclidean distance\n",
        "  - in one dimension between $u$ and $v$:\n",
        "  $$distance(u, v) = \\sqrt{(u - v)^2} = | u - v |$$\n",
        "\n",
        "  - in two dimensions between $u = <u_1, u_2>$ and $v = <v_1, v_2>$:\n",
        "  $$distance(u, v) = \\sqrt{(u_1 - v_1)^2 + (u_2 - v_2)^2}$$\n",
        "\n",
        "  - generally, in higher dimensions between vectors <br>\n",
        "  $u = <u_1, u_2, \\dots, u_n>$ and $v = <v_1, v_2, \\dots, v_n>$ is defined as:\n",
        "\n",
        "$$distance(u, v) = \\sqrt{\\sum_{i =1}^{n} (u_i - v_i)^2}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71FTS8oELRho"
      },
      "source": [
        "### Euclidean distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6MUQbNLLRho"
      },
      "outputs": [],
      "source": [
        "two_cities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ld1WWA0jLRho"
      },
      "source": [
        "- Subtract the two cities\n",
        "- Square the difference\n",
        "- Sum them up\n",
        "- Take the square root"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTW0ALPKLRho"
      },
      "outputs": [],
      "source": [
        "# Subtract the two cities\n",
        "print(\"Subtract the cities: \\n%s\\n\"\n",
        "      % (two_cities.iloc[1] - two_cities.iloc[0]))\n",
        "\n",
        "# Squared sum of the difference\n",
        "print(\"Sum of squares: %0.4f\\n\"\n",
        "      % (np.sum((two_cities.iloc[1] - two_cities.iloc[0]) ** 2)))\n",
        "\n",
        "# Take the square root\n",
        "print(\"Euclidean distance between cities: %0.4f\"\n",
        "      % (np.sqrt(np.sum((two_cities.iloc[1] - two_cities.iloc[0]) ** 2))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jk0kU983LRho"
      },
      "outputs": [],
      "source": [
        "two_cities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHzhHjtpLRho"
      },
      "outputs": [],
      "source": [
        "# Euclidean distance using sklearn\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "euclidean_distances(two_cities)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MDnuqsuLRhp"
      },
      "source": [
        "Note: `scikit-learn` supports a number of other [distance metrics](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.DistanceMetric.html).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3or65vILRhp"
      },
      "source": [
        "### Finding the nearest neighbour\n",
        "\n",
        "- Let's look at distances from all cities to all other cities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4_qiscRLRhp"
      },
      "outputs": [],
      "source": [
        "dists = euclidean_distances(X_cities)\n",
        "np.fill_diagonal(dists, np.inf)  # why is this needed?\n",
        "print(\"All distances: \", dists.shape)\n",
        "pd.DataFrame(dists)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co0jdsW-LRhp"
      },
      "source": [
        "Let's look at the distances between City 0 and some other cities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZuq3tCZLRhp"
      },
      "outputs": [],
      "source": [
        "print(\"Feature vector for city 0: \\n%s\\n\" % (X_cities.iloc[0]))\n",
        "print(\"Distances from city 0 to the first 5 cities: \\n%s\\n\" % (dists[0,:5]))\n",
        "# We can find the closest city with `np.argmin`:\n",
        "print(\"The closest city from city 0 is: %d \\nwith feature vector: \\n%s\"\n",
        "      % (np.argmin(dists[0]), X_cities.iloc[np.argmin(dists[0])]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYWlLxVNLRht"
      },
      "source": [
        "So, we **managed to find out the closest city to City 0, which is City 81**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwOcHi5ALRht"
      },
      "source": [
        "### Question\n",
        "\n",
        "- Why did we set the diagonal entries to infinity before finding the closest city?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBjHiCUyLRht"
      },
      "source": [
        "### Finding the distances to a query point\n",
        "\n",
        "We can also find the distances to a **new \"test\" or \"query\" city**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5cscRL9LRht"
      },
      "outputs": [],
      "source": [
        "# Let's find a city that's closest to a query city\n",
        "query_point = [[-80, 25]]\n",
        "\n",
        "dists = euclidean_distances(X_cities, query_point)\n",
        "dists[0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zf9bhjktLRht"
      },
      "outputs": [],
      "source": [
        "print(\"The query point %s is closest to the city with index %d \\n\"\n",
        "      \"and the distance between them is: %0.4f\"\n",
        "      % (query_point, np.argmin(dists), dists[np.argmin(dists)]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-wn1qDrLRhu"
      },
      "outputs": [],
      "source": [
        "# See the closest city (72) among some other cities with thir distances to query point\n",
        "X_cities.join(pd.DataFrame(dists, columns=['dist'])).head(np.argmin(dists) + 3).tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-O8v2fYLRhu"
      },
      "source": [
        "<br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkKOTgEoLRhu"
      },
      "source": [
        "## $k$-Nearest Neighbours ($k$-NNs) [[video](https://youtu.be/bENDqXKJLmg)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3zmJQywLRhu"
      },
      "outputs": [],
      "source": [
        "small_cities = cities_df.sample(30, random_state=90)\n",
        "one_city = small_cities.sample(1, random_state=44)\n",
        "# get all of small_cities excluding one_city:\n",
        "small_train_df = pd.concat([small_cities, one_city]).drop_duplicates(keep=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfVt179-LRhu"
      },
      "outputs": [],
      "source": [
        "X_small_cities = small_train_df[[\"longitude\", \"latitude\"]].to_numpy()\n",
        "y_small_cities = small_train_df[\"country\"].to_numpy()\n",
        "test_point = one_city[[\"longitude\", \"latitude\"]].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLd8MLoQLRhu"
      },
      "outputs": [],
      "source": [
        "plot_train_test_points(\n",
        "    X_small_cities,\n",
        "    y_small_cities,\n",
        "    test_point,\n",
        "    class_names=[\"Canada\", \"USA\"],\n",
        "    test_format=\"circle\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsLEyFKtLRhu"
      },
      "source": [
        "- Given a new data point, predict the class of the data point by finding the \"closest\" data point in the training set, i.e., by finding its \"nearest neighbour\" or majority vote of nearest neighbours."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tT0PYKs4LRhv"
      },
      "source": [
        "Suppose we want to predict the class of the black point.  \n",
        "- An intuitive way to do this is predict the same label as the \"closest\" point ($k = 1$) (1-nearest neighbour)\n",
        "- We would predict a target of **USA** in this case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gk94KIrELRhv"
      },
      "outputs": [],
      "source": [
        "plot_knn_clf(\n",
        "    X_small_cities,\n",
        "    y_small_cities,\n",
        "    test_point,\n",
        "    n_neighbors=1,\n",
        "    class_names=[\"Canada\", \"USA\"],\n",
        "    test_format=\"circle\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mVvj1L9LRhv"
      },
      "source": [
        "How about using $k > 1$ to get a more robust estimate?\n",
        "- For example, we could also use the 3 closest points (*k* = 3) and let them **vote** on the correct class.  \n",
        "- The **Canada** class would win in this case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "HbYqVX0_LRhv"
      },
      "outputs": [],
      "source": [
        "plot_knn_clf(\n",
        "    X_small_cities,\n",
        "    y_small_cities,\n",
        "    test_point,\n",
        "    n_neighbors=3,\n",
        "    class_names=[\"Canada\", \"USA\"],\n",
        "    test_format=\"circle\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B18UMhkMLRhv"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "k_values = [1, 3, 5]\n",
        "\n",
        "for k in k_values:\n",
        "    neigh = KNeighborsClassifier(n_neighbors=k)\n",
        "    neigh.fit(X_small_cities, y_small_cities)\n",
        "    print(\"Prediction of the black dot with %d neighbours: %s\"\n",
        "          % (k, neigh.predict(test_point)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bqFiQHpLRhv"
      },
      "source": [
        "### Choosing `n_neighbors`\n",
        "\n",
        "- The **primary hyperparameter** of the model is `n_neighbors` ($k$) which decides how many neighbours should vote during prediction?\n",
        "- What happens when we play around with `n_neighbors`?\n",
        "- Are we more likely to overfit with a low `n_neighbors` or a high `n_neighbors`?\n",
        "- Let's examine the effect of the hyperparameter on our cities data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7dVLsv4LRhw"
      },
      "outputs": [],
      "source": [
        "X = cities_df.drop(columns=[\"country\"])\n",
        "y = cities_df[\"country\"]\n",
        "\n",
        "# split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.1, random_state=123\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boh3vUIOLRhw"
      },
      "outputs": [],
      "source": [
        "k = 1\n",
        "knn1 = KNeighborsClassifier(n_neighbors=k)\n",
        "scores = cross_validate(knn1, X_train, y_train, return_train_score=True)\n",
        "pd.DataFrame(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYUiWm2fLRhw"
      },
      "outputs": [],
      "source": [
        "k = 100\n",
        "knn100 = KNeighborsClassifier(n_neighbors=k)\n",
        "scores = cross_validate(knn100, X_train, y_train, return_train_score=True)\n",
        "pd.DataFrame(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "UtZdOwwqLRhw"
      },
      "outputs": [],
      "source": [
        "def f(n_neighbors=1):\n",
        "    results = {}\n",
        "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
        "    scores = cross_validate(knn, X_train, y_train, return_train_score=True)\n",
        "    results[\"n_neighbours\"] = [n_neighbors]\n",
        "    results[\"mean_train_score\"] = [round(scores[\"train_score\"].mean(), 3)]\n",
        "    results[\"mean_valid_score\"] = [round(scores[\"test_score\"].mean(), 3)]\n",
        "    print(pd.DataFrame(results))\n",
        "\n",
        "\n",
        "interactive(\n",
        "    f,\n",
        "    n_neighbors=widgets.IntSlider(min=1, max=101, step=10, value=1),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HwpkCUFLRhw"
      },
      "outputs": [],
      "source": [
        "plot_knn_decision_boundaries(X_train, y_train, k_values=[1, 11, 100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7KREQ4XLRhw"
      },
      "source": [
        "### How to choose `n_neighbors`?\n",
        "\n",
        "- `n_neighbors` is a hyperparameter\n",
        "- We can use ***hyperparameter optimization*** to choose `n_neighbors`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjlnFrk3LRhx"
      },
      "outputs": [],
      "source": [
        "results_dict = {\n",
        "    \"n_neighbors\": [],\n",
        "    \"mean_train_error\": [],\n",
        "    \"mean_cv_error\": [],\n",
        "    \"std_cv_score\": [],\n",
        "    \"std_train_score\": [],\n",
        "}\n",
        "param_grid = {\"n_neighbors\": np.arange(1, 50, 5)}\n",
        "\n",
        "for k in param_grid[\"n_neighbors\"]:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    scores = cross_validate(knn, X_train, y_train, return_train_score=True)\n",
        "    results_dict[\"n_neighbors\"].append(k)\n",
        "\n",
        "    results_dict[\"mean_cv_error\"].append(1-np.mean(scores[\"test_score\"]))\n",
        "    results_dict[\"mean_train_error\"].append(1-np.mean(scores[\"train_score\"]))\n",
        "    results_dict[\"std_cv_score\"].append(scores[\"test_score\"].std())\n",
        "    results_dict[\"std_train_score\"].append(scores[\"train_score\"].std())\n",
        "\n",
        "results_df = pd.DataFrame(results_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHktnsx-LRhx"
      },
      "outputs": [],
      "source": [
        "results_df = results_df.set_index(\"n_neighbors\")\n",
        "results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "46sOTMobLRhx"
      },
      "outputs": [],
      "source": [
        "results_df[[\"mean_train_error\", \"mean_cv_error\"]].plot().invert_xaxis();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fr6ZrDILRhx"
      },
      "outputs": [],
      "source": [
        "best_n_neighbours = results_df.idxmin()[\"mean_cv_error\"]\n",
        "best_n_neighbours"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0pfJ9A1LRhx"
      },
      "source": [
        "Let's try our best model on test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "NX5LJw92LRhx"
      },
      "outputs": [],
      "source": [
        "knn = KNeighborsClassifier(n_neighbors=best_n_neighbours)\n",
        "knn.fit(X_train, y_train)\n",
        "print(\"Test accuracy: %0.3f\" % (knn.score(X_test, y_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_x1LQU3LRhy"
      },
      "source": [
        "<br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9gcGVDlLRhy"
      },
      "source": [
        "## More on $k$-NNs [[video](https://youtu.be/IRGbqi5S9gQ)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "mQZ-XSaBLRhy"
      },
      "source": [
        "### Other useful arguments of `KNeighborsClassifier`\n",
        "\n",
        "- `weights` $\\rightarrow$ When predicting label, you can assign higher weight to the examples which are closer to the query example.  \n",
        "- Exercise for you: Play around with this argument. Do you get a better validation score?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rCcc7E3LRhy"
      },
      "source": [
        "### Pros of $k$-NNs for supervised learning\n",
        "\n",
        "- Easy to understand, interpret.\n",
        "- Simple hyperparameter $k$ (`n_neighbors`) controlling the fundamental tradeoff.\n",
        "- Can learn very complex functions given enough data.\n",
        "- **Lazy learning**: Takes no time to `fit`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VuY39dPLRhy"
      },
      "source": [
        "### Cons of $k$-NNs for supervised learning\n",
        "\n",
        "- Can be potentially VERY **slow during prediction** time, especially when the training set is very large.\n",
        "- Often not that great test accuracy compared to the modern approaches.\n",
        "- It does not work well on datasets with many features or where most feature values are 0 most of the time (sparse datasets).    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNpLLXhDLRhy"
      },
      "source": [
        "***Important***\n",
        "> For regular $k$-NN for supervised learning (not with sparse matrices), you should scale your features. We'll be looking into it soon."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "1xyroqVFLRhz"
      },
      "source": [
        "### Parametric vs non parametric\n",
        "\n",
        "- You might see a lot of definitions of these terms.\n",
        "- A simple way to think about this is:\n",
        "    - *For $n$ samples, do you need to store at least $O(n)$* worth of stuff to make predictions? If so, it's non-parametric.\n",
        "- **Non-parametric example**: $k$-NN is a classic example of non-parametric models.     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MExe9M-ELRhz"
      },
      "source": [
        "***Note***\n",
        "> $\\mathcal{O}(n)$ is referred to as big $\\mathcal{O}$ notation. It tells you how fast an algorithm is or how much storage space it requires. For example, in simple terms, if you have $n$ samples and you need to store them all you can say that the algorithm requires $\\mathcal{O}(n)$ worth of stuff."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrnwuHS1LRhz"
      },
      "source": [
        "### Curse of dimensionality\n",
        "\n",
        "- Affects all learners but especially bad for nearest-neighbour.\n",
        "- $k$-NN usually works well when the number of dimensions $d$ is small but things fall apart quickly as $d$ goes up.\n",
        "- If there are **many irrelevant attributes, $k$-NN is hopelessly confused** because all of them contribute to finding similarity between examples.\n",
        "- With enough irrelevant attributes the accidental similarity swamps out meaningful similarity and $k$-NN is no better than random guessing.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9P4P-74HLRhz"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "\n",
        "nfeats_accuracy = {\"nfeats\": [], \"dummy_valid_accuracy\": [], \"KNN_valid_accuracy\": []}\n",
        "for n_feats in range(4, 2000, 100):\n",
        "    X, y = make_classification(n_samples=2000, n_features=n_feats, n_classes=2)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=123\n",
        "    )\n",
        "    dummy = DummyClassifier(strategy=\"most_frequent\")\n",
        "    dummy_scores = cross_validate(dummy, X_train, y_train, return_train_score=True)\n",
        "\n",
        "    knn = KNeighborsClassifier()\n",
        "    scores = cross_validate(knn, X_train, y_train, return_train_score=True)\n",
        "    nfeats_accuracy[\"nfeats\"].append(n_feats)\n",
        "    nfeats_accuracy[\"KNN_valid_accuracy\"].append(np.mean(scores[\"test_score\"]))\n",
        "    nfeats_accuracy[\"dummy_valid_accuracy\"].append(np.mean(dummy_scores[\"test_score\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WAE4DacLRhz"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(nfeats_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "317ZRDbwLRhz"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(nfeats_accuracy).set_index(\"nfeats\").plot();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fu9rBRa1LRh0"
      },
      "source": [
        "<br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ4wDC2nLRh0"
      },
      "source": [
        "## Summary\n",
        "\n",
        "- We have KNNs as new supervised learning techniques in our toolbox.\n",
        "- These are instance-based learners and the idea is to assign nearby points the same label.\n",
        "- Can be used for classification or regression."
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "ml2023fall",
      "language": "python",
      "name": "ml2023fall"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}